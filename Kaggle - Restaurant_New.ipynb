{
 "metadata": {
  "name": "",
  "signature": "sha256:8a072e7bf90ba84e5105338a769777de82beba2ff8b9461f6e2132b471c51721"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# -*- coding: utf-8 -*-\n",
      "\"\"\"\n",
      "Created on Tue Mar 24 13:56:08 2015\n",
      "\n",
      "@author: dpryce\n",
      "\"\"\"\n",
      "import os\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "import numpy as np\n",
      "import numpy as np\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.linear_model import ElasticNetCV\n",
      "from sklearn.linear_model import LassoCV\n",
      "from sklearn import linear_model\n",
      "from sklearn import *\n",
      "from sklearn.ensemble import GradientBoostingRegressor\n",
      "from sklearn import svm\n",
      "from sklearn import metrics\n",
      "from sklearn.ensemble import RandomForestRegressor\n",
      "import csv\n",
      "np.random.seed(0)\n",
      "\n",
      "'''DATA CHECKS'''\n",
      "#print train_df.dtypes\n",
      "##identified date, city, city group and type that need munging/cleaning\n",
      "## also all values need to set to float before modelling\n",
      "#print train_df.info()\n",
      "## identified there are no null values but must check if zero values mean anything\n",
      "\n",
      "def munge_rest(df):    \n",
      "    '''DROP ID'''\n",
      "    df = df.drop(['Id'],axis=1)\n",
      "    \n",
      "\n",
      "    \n",
      "    '''MUNGE DATE FEATURES'''\n",
      "    #Create new column for date and copy from open date into pd datetime\n",
      "    df['Date'] = df['Open Date'].apply(pd.to_datetime)\n",
      "    \n",
      "    ##Use the below to set the index as date instead of dataframe id\n",
      "    df.set_index('Date', inplace=True)\n",
      "    \n",
      "    ##Extract variables from date\n",
      "    df['OpenDay'] = df.index.day\n",
      "    df['OpenMonth'] = df.index.month\n",
      "    df['OpenYear'] = df.index.year\n",
      "    df = df.reset_index(drop=True)\n",
      "    \n",
      "    #Drop the open date column\n",
      "    df = df.drop(['Open Date'],axis=1)\n",
      "    \n",
      "    '''MUNGE CITY FEATURE'''\n",
      "    #creating a dictionary for the cities each unique city has a value\n",
      "    unique_cities = pd.unique(df.City.ravel())\n",
      "    values = np.arange(len(unique_cities))\n",
      "    city_dict = dict(zip(unique_cities,values))\n",
      "    #\u0130stanbul: 0, Ankara: 1, Diyarbak\u0131r:2, Tokat: 3, Gaziantep: 4, \n",
      "    #Afyonkarahisar 5, Edirne: 6, Kocaeli: 7, Bursa: 8, \u0130zmir: 9,\n",
      "    #Sakarya: 10, Elaz\u0131\u011f: 11, Kayseri: 12, Eski\u015fehir: 13, \u015eanl\u0131urfa: 14,\n",
      "    #Samsun: 15, Adana: 16, Antalya: 17, Kastamonu: 18, U\u015fak: 19, Mu\u011fla: 20\n",
      "    #K\u0131rklareli: 21, Konya: 22, Karab\u00fck: 23, Tekirda\u011f: 24, Denizli: 25\n",
      "    #Bal\u0131kesir: 26, Ayd\u0131n: 27, Amasya: 28, K\u00fctahya: 29, Bolu: 30, Trabzon: 31\n",
      "    #Isparta: 32, Osmaniye: 33\n",
      "    \n",
      "    #NOTE: \"Tan\u0131ms\u0131z\" means undefined in Turkish - language used here\n",
      "    \n",
      "    #Adding new column\n",
      "    df['CityID']=df['City']\n",
      "    \n",
      "    #Replacing all city strings with dictionary values\n",
      "    df=df.replace({\"CityID\": city_dict})\n",
      "    \n",
      "    #Drop the city string column\n",
      "    df = df.drop(['City'],axis=1)\n",
      "    \n",
      "    \n",
      "    '''MUNGE CITY GROUP FEATURE'''\n",
      "    # City Group: Type of the city. Big cities, or Other. \n",
      "    #Create a dictionary for this\n",
      "    group_dict = {\"Big Cities\": 0, \"Other\": 1}\n",
      "    #create a new column to work on\n",
      "    df['GroupID']=df['City Group']\n",
      "    #replace values from dictionary\n",
      "    df = df.replace({'GroupID':group_dict})\n",
      "    ##check the two columns agree\n",
      "    #print df[['City Group','GroupID']]\n",
      "    #Drop the city group string column\n",
      "    df = df.drop(['City Group'],axis=1)\n",
      "    \n",
      "    '''MUNGE TYPE FEATURE'''\n",
      "    #Type: Type of the restaurant. FC: Food Court, IL: Inline, DT: Drive Thru, MB: Mobile\n",
      "    #Create dictionary for this\n",
      "    type_dict = {\"FC\": 0, \"IL\": 1, \"DT\": 2, \"MB\": 3}\n",
      "    #copy column to work on\n",
      "    df['TypeID']=df['Type']\n",
      "    #replace values from dictionary\n",
      "    df = df.replace({'TypeID':type_dict})\n",
      "    ##check the two columns agree\n",
      "    #print df[['Type','TypeID']]\n",
      "    #Drop the Type string column\n",
      "    df = df.drop(['Type'],axis=1)\n",
      "    \n",
      "    #Combine Numeric variables and binary\n",
      "    \n",
      "    '''FINAL DATA CHECKS'''\n",
      "    #print df.dtypes\n",
      "    ##all values are int or float\n",
      "    #print df.info()\n",
      "    # same as before need to invesigate if zero means anything\n",
      "    \n",
      "    #print train_values\n",
      "    \n",
      "    #drop dataframe in a numpy array\n",
      "    data = df.values\n",
      "    return data\n",
      "\n",
      "#change below for your local file directory \n",
      "os.chdir('C:\\Users\\ssbizzera\\Documents\\Data Analytics\\Kaggle\\Restaurant')\n",
      "\n",
      "'''DATA INPUT'''\n",
      "#input training data into initial pandas dataframe\n",
      "train_df = pd.read_csv('train.csv',sep=\",\")\n",
      "test_df = pd.read_csv('test.csv',sep=\",\")   \n",
      "    \n",
      "train_data = munge_rest(train_df)\n",
      "test_data = munge_rest(test_df)    \n",
      "\n",
      "#Delete revenue column from train data\n",
      "x = np.delete(train_data,37,1)\n",
      "revenue = train_data[:,37]\n",
      "\n",
      "'''TRAINING'''\n",
      "# Create the random forest object which will include all the parameters\n",
      "# for the fit\n",
      "forest = GradientBoostingRegressor(n_estimators = 45,max_depth=None,max_features='sqrt')\n",
      "Elastic= ElasticNetCV(eps=0.01, n_alphas=130, l1_ratio=0.2,alphas=None, fit_intercept=True, normalize=True, precompute='auto', max_iter=10000, tol=0.001, copy_X=True, cv=4, verbose=False, n_jobs=3, positive=False)\n",
      "\n",
      "alphas = np.logspace(-10, -1, 340)\n",
      "\n",
      "clf = linear_model.Ridge(alpha=0.088, fit_intercept=True, normalize=True, copy_X=True, max_iter=10000, tol=0.015, solver='auto')\n",
      "\n",
      "#l1_ratio=0.65\n",
      "# Fit the training data to the revenue and create the decision trees\n",
      "Ela = clf.fit(x,train_data[0::,37])\n",
      "\n",
      "#prints the oob score -- I think\n",
      "#print forest.score(train_data[0::,1::],train_data[0::,37])\n",
      "\n",
      "'''TESTING'''\n",
      "## Take the same decision trees and run it on the test data\n",
      "output = Ela.predict(test_data)\n",
      "    \n",
      "    \n",
      "''' PRINT TO FILE'''\n",
      "predictions_file = open(\"restaurants_rf.csv\", \"wb\")\n",
      "p = csv.writer(predictions_file)\n",
      "p.writerow([\"Id\", \"Prediction\"])\n",
      "\n",
      "for i in range(0,len(output)):\n",
      "    p.writerow([i, output[i]])\n",
      "\n",
      "predictions_file.close()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "    Numeric_features = [c for c in df if df[c].dtype.kind in ('f','i')]\n",
      "    if revenue in Numeric_features: Numeric_features.remove('revenue')\n",
      "    \n",
      "    #Scale\n",
      "    df_num = df[Numeric_features].apply(lambda x:(x.astype(float) - min(x))/(max(x)-min(x)), axis = 0)\n",
      "    \n",
      "    Numeric_features = [c for c in df if df[c].dtype.kind in ('f','i')]\n",
      "    if revenue in Numeric_features: Numeric_features.remove('revenue')\n",
      "\n",
      "\n",
      "    #Log transformation\n",
      "    df = df.reset_index(drop=True)\n",
      "    df_log = df[Numeric_features].apply(np.log)\n",
      "\n",
      "    #Replace inf by 0\n",
      "    df_log = df_log.replace([np.inf, -np.inf], 0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''TRAINING'''\n",
      "# Create the random forest object which will include all the parameters\n",
      "# for the fit\n",
      "forest = GradientBoostingRegressor(n_estimators = 45,max_depth=None,max_features='sqrt')\n",
      "Elastic= ElasticNetCV(eps=0.01, n_alphas=130, l1_ratio=0.2,alphas=None, fit_intercept=True, normalize=True, precompute='auto', max_iter=10000, tol=0.001, copy_X=True, cv=4, verbose=False, n_jobs=3, positive=False)\n",
      "\n",
      "alphas = np.logspace(-10, -1, 340)\n",
      "\n",
      "clf = linear_model.Ridge(alpha=0.068, fit_intercept=True, normalize=True, copy_X=True, max_iter=10000, tol=0.009, solver='auto')\n",
      "\n",
      "#l1_ratio=0.65\n",
      "# Fit the training data to the revenue and create the decision trees\n",
      "Ela = clf.fit(x,train_data[0::,37])\n",
      "\n",
      "#prints the oob score -- I think\n",
      "#print forest.score(train_data[0::,1::],train_data[0::,37])\n",
      "==> 0.068 = 1805082.21068\n",
      "==> 0.081 = 1789496.58586"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}